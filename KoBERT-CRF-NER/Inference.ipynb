{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9O4Dl0mJMRs",
        "outputId": "0c02ebc6-f6df-420e-9a96-4f2294f9acd2"
      },
      "source": [
        "!git clone https://github.com/eagle705/pytorch-bert-crf-ner.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-bert-crf-ner'...\n",
            "remote: Enumerating objects: 1725, done.\u001b[K\n",
            "remote: Counting objects: 100% (1725/1725), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1636/1636), done.\u001b[K\n",
            "remote: Total 1725 (delta 118), reused 1662 (delta 71), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1725/1725), 27.16 MiB | 31.50 MiB/s, done.\n",
            "Resolving deltas: 100% (118/118), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDQCT-8xJQSt",
        "outputId": "c722da09-b791-4761-e863-44724e0626df"
      },
      "source": [
        "# 저장소로 이동\n",
        "%cd pytorch-bert-crf-ner"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-bert-crf-ner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jjTz4JKJRjx"
      },
      "source": [
        "# !pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znSbWu6hPsQv"
      },
      "source": [
        "!pip install pytorch-crf \n",
        "!pip install pytorch-transformers==1.2.0\n",
        "!pip install transformers==2.1.1  # https://github.com/SKTBrain/KoBERT/issues/31\n",
        "!pip install torch==1.8.1\n",
        "!pip install torchvision==0.9.1\n",
        "!pip install gluonnlp\n",
        "!pip install mxnet==1.8.0.post0\n",
        "!pip install konlpy==0.5.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy4VI-PiJVXF",
        "outputId": "9ea05ab3-99a6-4d63-bc5b-a99eced3249a"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import json\n",
        "import pickle\n",
        "import argparse\n",
        "import torch\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "from kobert.utils import get_tokenizer\n",
        "from model.net import KobertSequenceFeatureExtractor, KobertCRF, KobertBiLSTMCRF, KobertBiGRUCRF\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "from data_utils.utils import Config\n",
        "from data_utils.vocab_tokenizer import Tokenizer\n",
        "from data_utils.pad_sequence import keras_pad_fn\n",
        "from pathlib import Path"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWxX1WNPpEx7"
      },
      "source": [
        "## Train_Bert_CRF에서 학습 완료된 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-Kli7sAowVk",
        "outputId": "3541325f-e678-45fc-b490-4690c090eba4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4TucCeErQ_z"
      },
      "source": [
        "1. 드라이브에 base_model_with_crf_val 폴더를 생성하고 학습된 모델 best-epoch-12-step-1000-acc-0.960.bin 을 업로드\n",
        "\n",
        "2. git clone한 저장소의 base_model_with_crf에 있는 config.json, ner_to_index.json을 다운 받아서 드라이브에 생성된 폴더에 업로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42UTCbGfr4vd"
      },
      "source": [
        "class DecoderFromNamedEntitySequence():\n",
        "    def __init__(self, tokenizer, index_to_ner):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.index_to_ner = index_to_ner\n",
        "\n",
        "    def __call__(self, list_of_input_ids, list_of_pred_ids):\n",
        "        input_token = self.tokenizer.decode_token_ids(list_of_input_ids)[0]\n",
        "        pred_ner_tag = [self.index_to_ner[pred_id] for pred_id in list_of_pred_ids[0]]\n",
        "\n",
        "        print(\"len: {}, input_token:{}\".format(len(input_token), input_token))\n",
        "        print(\"len: {}, pred_ner_tag:{}\".format(len(pred_ner_tag), pred_ner_tag))\n",
        "\n",
        "        # ----------------------------- parsing list_of_ner_word ----------------------------- #\n",
        "        list_of_ner_word = []\n",
        "        entity_word, entity_tag, prev_entity_tag = \"\", \"\", \"\"\n",
        "        for i, pred_ner_tag_str in enumerate(pred_ner_tag):\n",
        "            if \"B-\" in pred_ner_tag_str:\n",
        "                entity_tag = pred_ner_tag_str[-3:]\n",
        "\n",
        "                if prev_entity_tag != entity_tag and prev_entity_tag != \"\":\n",
        "                    list_of_ner_word.append({\"word\": entity_word.replace(\"▁\", \" \"), \"tag\": prev_entity_tag, \"prob\": None})\n",
        "\n",
        "                entity_word = input_token[i]\n",
        "                prev_entity_tag = entity_tag\n",
        "            elif \"I-\"+entity_tag in pred_ner_tag_str:\n",
        "                entity_word += input_token[i]\n",
        "            else:\n",
        "                if entity_word != \"\" and entity_tag != \"\":\n",
        "                    list_of_ner_word.append({\"word\":entity_word.replace(\"▁\", \" \"), \"tag\":entity_tag, \"prob\":None})\n",
        "                entity_word, entity_tag, prev_entity_tag = \"\", \"\", \"\"\n",
        "\n",
        "\n",
        "        # ----------------------------- parsing decoding_ner_sentence ----------------------------- #\n",
        "        decoding_ner_sentence = \"\"\n",
        "        is_prev_entity = False\n",
        "        prev_entity_tag = \"\"\n",
        "        is_there_B_before_I = False\n",
        "\n",
        "        for token_str, pred_ner_tag_str in zip(input_token, pred_ner_tag):\n",
        "            token_str = token_str.replace('▁', ' ')  # '▁' 토큰을 띄어쓰기로 교체\n",
        "\n",
        "            if 'B-' in pred_ner_tag_str:\n",
        "                if is_prev_entity is True:\n",
        "                    decoding_ner_sentence += ':' + prev_entity_tag+ '>'\n",
        "\n",
        "                if token_str[0] == ' ':\n",
        "                    token_str = list(token_str)\n",
        "                    token_str[0] = ' <'\n",
        "                    token_str = ''.join(token_str)\n",
        "                    decoding_ner_sentence += token_str\n",
        "                else:\n",
        "                    decoding_ner_sentence += '<' + token_str\n",
        "                is_prev_entity = True\n",
        "                prev_entity_tag = pred_ner_tag_str[-3:] # 첫번째 예측을 기준으로 하겠음\n",
        "                is_there_B_before_I = True\n",
        "\n",
        "            elif 'I-' in pred_ner_tag_str:\n",
        "                decoding_ner_sentence += token_str\n",
        "\n",
        "                if is_there_B_before_I is True: # I가 나오기전에 B가 있어야하도록 체크\n",
        "                    is_prev_entity = True\n",
        "            else:\n",
        "                if is_prev_entity is True:\n",
        "                    decoding_ner_sentence += ':' + prev_entity_tag+ '>' + token_str\n",
        "                    is_prev_entity = False\n",
        "                    is_there_B_before_I = False\n",
        "                else:\n",
        "                    decoding_ner_sentence += token_str\n",
        "\n",
        "        return list_of_ner_word, decoding_ner_sentence"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjuYmHZhJxEY"
      },
      "source": [
        "vocab_dir = Path('experiments/base_model_with_crf_val')\n",
        "\n",
        "model_dir = Path('/content/drive/MyDrive/base_model_with_crf_val')\n",
        "model_config = Config(json_path=model_dir / 'config.json')\n",
        "\n",
        "# Vocab & Tokenizer\n",
        "# tok_path = get_tokenizer() # ./tokenizer_78b3253a26.model\n",
        "tok_path = \"ptr_lm_model/tokenizer_78b3253a26.model\"\n",
        "ptr_tokenizer = SentencepieceTokenizer(tok_path)\n",
        "\n",
        "# load vocab & tokenizer\n",
        "with open(vocab_dir / \"vocab.pkl\", 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "tokenizer = Tokenizer(vocab=vocab, split_fn=ptr_tokenizer, pad_fn=keras_pad_fn, maxlen=model_config.maxlen)\n",
        "\n",
        "# load ner_to_index.json\n",
        "with open(model_dir / \"ner_to_index.json\", 'rb') as f:\n",
        "    ner_to_index = json.load(f)\n",
        "    index_to_ner = {v: k for k, v in ner_to_index.items()}\n",
        "\n",
        "# Model\n",
        "# model = KobertSequenceFeatureExtractor(config=model_config, num_classes=len(ner_to_index))\n",
        "model = KobertCRF(config=model_config, num_classes=len(ner_to_index), vocab=vocab)\n",
        "# model = KobertBiLSTMCRF(config=model_config, num_classes=len(ner_to_index), vocab=vocab)\n",
        "# model = KobertBiGRUCRF(config=model_config, num_classes=len(ner_to_index), vocab=vocab)\n",
        "\n",
        "# load\n",
        "model_dict = model.state_dict()\n",
        "# checkpoint = torch.load(\"./experiments/base_model/best-epoch-9-step-600-acc-0.845.bin\", map_location=torch.device('cpu'))\n",
        "# checkpoint = torch.load(\"./experiments/base_model_with_crf/best-epoch-16-step-1500-acc-0.993.bin\", map_location=torch.device('cpu'))\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/base_model_with_crf_val/best-epoch-12-step-1000-acc-0.960.bin\", map_location=torch.device('cpu'))\n",
        "# checkpoint = torch.load(\"./experiments/base_model_with_bilstm_crf/best-epoch-15-step-2750-acc-0.992.bin\", map_location=torch.device('cpu'))\n",
        "# checkpoint = torch.load(\"./experiments/base_model_with_bigru_crf/model-epoch-18-step-3250-acc-0.997.bin\", map_location=torch.device('cpu'))\n",
        "\n",
        "convert_keys = {}\n",
        "for k, v in checkpoint['model_state_dict'].items():\n",
        "    new_key_name = k.replace(\"module.\", '')\n",
        "    if new_key_name not in model_dict:\n",
        "        print(\"{} is not int model_dict\".format(new_key_name))\n",
        "        continue\n",
        "    convert_keys[new_key_name] = v\n",
        "\n",
        "model.load_state_dict(convert_keys)\n",
        "model.eval()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# n_gpu = torch.cuda.device_count()\n",
        "# if n_gpu > 1:\n",
        "#     model = torch.nn.DataParallel(model)\n",
        "\n",
        "decoder_from_res = DecoderFromNamedEntitySequence(tokenizer=tokenizer, index_to_ner=index_to_ner)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdZvDCLxsEOv"
      },
      "source": [
        "## 새로운 문장으로 실험"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svetMq0ovF53"
      },
      "source": [
        "while(True):\n",
        "    input_text = input(\"문장을 입력하세요: \")\n",
        "    list_of_input_ids = tokenizer.list_of_string_to_list_of_cls_sep_token_ids([input_text])\n",
        "    x_input = torch.tensor(list_of_input_ids).long()\n",
        "\n",
        "    ## for bert alone\n",
        "    # y_pred = model(x_input)\n",
        "    # list_of_pred_ids = y_pred.max(dim=-1)[1].tolist()\n",
        "\n",
        "    ## for bert crf\n",
        "    list_of_pred_ids = model(x_input)\n",
        "\n",
        "    ## for bert bilstm crf & bert bigru crf\n",
        "    # list_of_pred_ids = model(x_input, using_pack_sequence=False)\n",
        "\n",
        "    list_of_ner_word, decoding_ner_sentence = decoder_from_res(list_of_input_ids=list_of_input_ids, list_of_pred_ids=list_of_pred_ids)\n",
        "    print(\"list_of_ner_word:\", list_of_ner_word)\n",
        "    print(\"decoding_ner_sentence:\", decoding_ner_sentence)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT2oAWTBvCJ_"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/44194558/139576616-1cbeb6c0-dcef-46b8-90e8-d8c3a7fcf5b2.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPXFiLC5sNwH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}