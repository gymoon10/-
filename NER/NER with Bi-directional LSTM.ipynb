{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER with Bi-directional LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4inwiDxarR6",
        "outputId": "a9858694-e70f-46d7-c0df-7a95af6c33d0"
      },
      "source": [
        "!pip install keras==2.2.5\n",
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.2.5\n",
            "  Downloading Keras-2.2.5-py2.py3-none-any.whl (336 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 51 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 61 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 71 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 92 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 102 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 112 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 122 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 133 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 143 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 153 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 163 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 174 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 184 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 194 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 204 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 215 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 225 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 235 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 245 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 256 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 266 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 276 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 286 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 296 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 307 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 317 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 327 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 336 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.5) (1.5.2)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.6.0\n",
            "    Uninstalling keras-2.6.0:\n",
            "      Successfully uninstalled keras-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires keras~=2.6, but you have keras 2.2.5 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.2.5 keras-applications-1.0.8\n",
            "Collecting tensorflow==1.14.0\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 52 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.37.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 69.5 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.4.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.17.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.41.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.8.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.6.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAXFExGNaxF0",
        "outputId": "a3f47d8a-9709-40ac-efe6-5042de19d6f5"
      },
      "source": [
        "import tensorflow\n",
        "import keras\n",
        "print(tensorflow.__version__)\n",
        "print(\"keras version:\", keras.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.14.0\n",
            "keras version: 2.2.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slU3ro5na_ss"
      },
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDc0EkY3bCi3",
        "outputId": "cbec7891-ed51-48d7-d44c-ed4592eacd9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msvuXr18c4_M"
      },
      "source": [
        "train.txt\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/44194558/137693162-d51fb4c2-416b-4325-a33b-21daff3e0f05.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv63HF-agsyj"
      },
      "source": [
        "### 데이터 불러오기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXGUw6hKbq2q"
      },
      "source": [
        "sentences = []\n",
        "buff_sentence = [] # 단어, 개체명 태깅 정보\n",
        "\n",
        "with open('/content/drive/My Drive/train.txt', 'r') as f_r:\n",
        "    # 문장 불러오기\n",
        "    data = f_r.readlines()  \n",
        "    # 각 문장에 대한 처리\n",
        "    for sentence in data:\n",
        "        if len(sentence) == 0 or sentence.startswith('-DOCSTART-') or sentence[0] == \"\\n\":\n",
        "            if len(buff_sentence) > 0:\n",
        "                sentences.append(buff_sentence)\n",
        "                buff_sentence = []\n",
        "            continue\n",
        "\n",
        "        splits = sentence.split(' ')\n",
        "        splits[-1] = re.sub(r'\\n', '', splits[-1]) # 줄바꿈 표시 \\n 제거\n",
        "        word = splits[0].lower() # 단어들은 소문자로 바꿔서 저장\n",
        "        buff_sentence.append([word, splits[-1]]) "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBN1qng3cjKi",
        "outputId": "a3c5b8cf-3cf5-455b-b4fb-60aa148cc455"
      },
      "source": [
        "print(sentences[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['eu', 'B-ORG'], ['rejects', 'O'], ['german', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['british', 'B-MISC'], ['lamb', 'O'], ['.', 'O']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxpmG-_rcubx"
      },
      "source": [
        "sentences_info = [] # 단어 변수\n",
        "tags_info = [] # 태그 변수"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97UXPzOqdclt"
      },
      "source": [
        "for sent in sentences:\n",
        "    word, tag = zip(*sent) # * : unlist\n",
        "    sentences_info.append(list(word))\n",
        "    tags_info.append(list(tag))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFnbsI_0dk61",
        "outputId": "962be34e-4aae-4a23-c2fe-882567af4aa0"
      },
      "source": [
        "print(*sentences[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eu', 'B-ORG'] ['rejects', 'O'] ['german', 'B-MISC'] ['call', 'O'] ['to', 'O'] ['boycott', 'O'] ['british', 'B-MISC'] ['lamb', 'O'] ['.', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4_jJl8NdnkX",
        "outputId": "f0d8e031-3ae6-4e7f-f84a-b21d0f2778ce"
      },
      "source": [
        "print(\"첫 번째 문장:\", sentences_info[0])\n",
        "print(\"첫 번째 태그 정보:\", tags_info[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 문장: ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
            "첫 번째 태그 정보: ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8iMv-uLeEBf",
        "outputId": "27aed9d8-a95a-49fc-91d1-8e6d9a32d8cd"
      },
      "source": [
        "print(\"열 한 번째 문장:\", sentences_info[10])\n",
        "print(\"열 한 번째 태그 정보:\",tags_info[10])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "열 한 번째 문장: ['spanish', 'farm', 'minister', 'loyola', 'de', 'palacio', 'had', 'earlier', 'accused', 'fischler', 'at', 'an', 'eu', 'farm', 'ministers', \"'\", 'meeting', 'of', 'causing', 'unjustified', 'alarm', 'through', '\"', 'dangerous', 'generalisation', '.', '\"']\n",
            "열 한 번째 태그 정보: ['B-MISC', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7zoO-bMgz7b"
      },
      "source": [
        "### 전처리 : Tokenize & Sequence 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Hvoyv8eH-G"
      },
      "source": [
        "vocab_size = 5000  # 5000개의 단어만 사용\n",
        "src_tokenizer = Tokenizer(num_words=vocab_size, oov_token='OOV')  # 문장 데이터\n",
        "src_tokenizer.fit_on_texts(sentences_info)\n",
        "\n",
        "tar_tokenizer = Tokenizer() # 태그 데이터\n",
        "tar_tokenizer.fit_on_texts(tags_info)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab93j3Tcen2Z"
      },
      "source": [
        "# 시퀀스 변환 (단어 인덱스 집합)\n",
        "x_train = src_tokenizer.texts_to_sequences(sentences_info)\n",
        "y_train = tar_tokenizer.texts_to_sequences(tags_info)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbbK6Oz7er8b",
        "outputId": "6611aeee-705a-43fe-bfdb-d4b13b671e0c"
      },
      "source": [
        "print('원본 문장 :', sentences_info[0])\n",
        "print('시퀀스로 변환된 문장 :', x_train[0])\n",
        "\n",
        "print('원본 태그 :', tags_info[0])\n",
        "print('시퀀스로 변환된 태그 :', y_train[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본 문장 : ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
            "시퀀스로 변환된 문장 : [989, 1, 205, 629, 7, 3939, 216, 1, 3]\n",
            "원본 태그 : ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
            "시퀀스로 변환된 태그 : [4, 1, 7, 1, 1, 1, 7, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78-6sTsjetCd",
        "outputId": "40dff49a-5fd3-4e27-94fe-f2bbe2bbfe84"
      },
      "source": [
        "index2word = src_tokenizer.index_word  # 단어 인덱스를 저장하는 딕셔너리\n",
        "index2tag = tar_tokenizer.index_word\n",
        "\n",
        "decoded = []\n",
        "\n",
        "for index in x_train[0]:\n",
        "    decoded.append(index2word[index])\n",
        "\n",
        "print(\"기존 문장: {}\".format(sentences_info[0]))\n",
        "print(\"Vocabulary에 없어 OOV 처리된 단어: {}\".format(decoded))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 문장: ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
            "Vocabulary에 없어 OOV 처리된 단어: ['eu', 'OOV', 'german', 'call', 'to', 'boycott', 'british', 'OOV', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU-DnqCwevmB",
        "outputId": "f1e88dc8-4eb4-41fd-ebca-73a4e9515d1e"
      },
      "source": [
        "# 고정된 길이의 시퀀스로 변환\n",
        "max_len = 80\n",
        "x_train_padded = pad_sequences(x_train, padding='post', maxlen=max_len)\n",
        "y_train_padded = pad_sequences(y_train, padding='post', maxlen=max_len)\n",
        "\n",
        "print(x_train_padded[0])\n",
        "print(y_train_padded[0])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 989    1  205  629    7 3939  216    1    3    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n",
            "[4 1 7 1 1 1 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgxJSisTfJDs",
        "outputId": "35b4b4e7-d5b3-4973-d171-76bca6809c49"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_train_padded, y_train_padded, test_size=.2,\n",
        "random_state=555)\n",
        "\n",
        "print(len(x_train))\n",
        "print(len(y_train))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11232\n",
            "11232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krz1ZC67hOGu",
        "outputId": "c3e8b6ad-5bfa-4b19-c579-0b8b358b9627"
      },
      "source": [
        "print(y_train[0].shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmxHyv_1fazg"
      },
      "source": [
        "tag_size = len(tar_tokenizer.word_index) + 1\n",
        "y_train = to_categorical(y_train, num_classes=tag_size)\n",
        "y_test = to_categorical(y_test, num_classes=tag_size)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YU_R3XDhM2w",
        "outputId": "be5a5666-dd7f-4075-97ed-c6828e08333b"
      },
      "source": [
        "print(y_train[0].shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1apEeDUhR4b",
        "outputId": "f6fdf8c0-4373-4cac-80d8-a93deb1dfca2"
      },
      "source": [
        "print('훈련 샘플 문장의 크기: {}'.format(x_train.shape))\n",
        "print('훈련 샘플 레이블의 크기: {}'.format(y_train.shape))\n",
        "print('테스트 샘플 문장의 크기: {}'.format(x_test.shape))\n",
        "print('테스트 샘플 레이블의 크기: {}'.format(y_test.shape))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 샘플 문장의 크기: (11232, 80)\n",
            "훈련 샘플 레이블의 크기: (11232, 80, 10)\n",
            "테스트 샘플 문장의 크기: (2809, 80)\n",
            "테스트 샘플 레이블의 크기: (2809, 80, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRMWfldIxDOd"
      },
      "source": [
        "### 모델링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4s6IG2xhXSd",
        "outputId": "a3a6b638-2216-4ba2-8797-5e8cdb254b7b"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "# mask_zero=True : 패딩된 값을 마스킹하여 네트워크의 뒤로 전달되지 않도록 (인위적으로 패딩된 부분이 학습에 영향을 미치지 않게 하도록)\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len, mask_zero=True))  \n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
        "model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 80, 128)           640000    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 80, 512)           788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 80, 10)            5130      \n",
            "=================================================================\n",
            "Total params: 1,433,610\n",
            "Trainable params: 1,433,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4dLj4dPijOf"
      },
      "source": [
        "embedding 계층의 출력은 (batch_size, max_len, output_dim = embedding_dim) = (None, 80, 128)\n",
        "\n",
        "5000개 단어(vocab_size)를 128 차원의 임베딩 노드로 전환 : 128 x 5000 = 64000\n",
        "\n",
        "788480 = (256 x (128 + 256) + 256) x 4 x 2 = (LSTM 노드 수 x (입력 임베딩 차원 + LSTM 노드 수) + bias) x gate 수 x 2 (양방향)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a-xTwv3hu6L",
        "outputId": "d15c20dc-5e42-4082-a652-44f22a48b511"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy-rE7NdilrJ",
        "outputId": "ab0d9306-be44-40ab-f1fe-4d27cdf26302"
      },
      "source": [
        "hist = model.fit(x_train, y_train, batch_size=128, epochs=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 11232 samples, validate on 2809 samples\n",
            "Epoch 1/1\n",
            "11232/11232 [==============================] - 154s 14ms/step - loss: 0.8959 - acc: 0.8242 - val_loss: 0.5854 - val_acc: 0.8299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dro6qgXxHY4"
      },
      "source": [
        "### 결과"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZHu1wBhi8ZV",
        "outputId": "f68bd4fc-6663-4dd5-b0b2-aed341cc61cd"
      },
      "source": [
        "print(\"train loss: \", hist.history['loss'])\n",
        "print(\"train accuracy: \", hist.history['acc'])\n",
        "print(\"val loss: \", hist.history['val_loss'])\n",
        "print(\"val accuracy: \", hist.history['val_acc'])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:  [0.8959357295280848]\n",
            "train accuracy:  [0.8241946862803565]\n",
            "val loss:  [0.5854463567297499]\n",
            "val accuracy:  [0.8299064717075502]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptm9vRHfxJ9S",
        "outputId": "10d3e50f-fb61-446d-fd28-eaa6d1198486"
      },
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(x_test, y_test)[1]))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2809/2809 [==============================] - 18s 6ms/step\n",
            "\n",
            " 테스트 정확도: 0.8294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh1Rs_EhxKH-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}